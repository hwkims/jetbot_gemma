## 유튜브 영상 대본:  🤖 JetBot 원격 제어 + 실시간 상황 인식 (FastAPI, Ollama, Edge TTS)

**영상 길이:** 약 10-15분

**대상 시청자:**  Jetson Nano, JetBot, FastAPI, Ollama, TTS 등에 관심 있는 개발자, 학생, 메이커

**목표:**  JetBot 프로젝트를 소개하고,  시청자들이 직접 따라 할 수 있도록 단계별로 설명

**영상 구성:**

**(0:00 - 0:30) 인트로**

*   화면:  JetBot이 움직이는 모습,  웹 인터페이스 화면,  Ollama 로고, FastAPI 로고, Edge TTS 로고 등이 빠르게 지나감.
*   음악:  경쾌하고 신나는 배경 음악.
*   내레이션:
    *   "안녕하세요!  오늘은 인공지능의 눈과 귀를 가진 로봇, JetBot을 만들어 볼 겁니다!"
    *   "JetBot, Ollama, FastAPI, Edge TTS를 활용해서 원격으로 JetBot을 제어하고, 주변 상황을 실시간으로 인식하는 시스템을 구축할 거예요."
    *   "젯슨 나노(Jetson Nano) 기반의 JetBot이 카메라로 주변을 보고, Ollama가 이걸 분석해서, FastAPI 서버를 통해 PC로 전달하고,  Edge TTS가 음성으로 상황을 설명해 주는 거죠!"
    *   "코딩 초보도 따라 할 수 있도록 쉽게 설명해 드릴 테니, 걱정 말고 함께 만들어 봐요!"

**(0:30 - 1:30) 프로젝트 개요 및 데모**

*   화면:  JetBot 웹 인터페이스 (웹캠 영상, 버튼, 응답 텍스트, 오디오 플레이어)
*   내레이션:
    *   "먼저 완성된 프로젝트를 간단하게 보여드릴게요.  이게 바로 저희가 만들 JetBot 제어 센터입니다!"
    *   (웹캠 영상 가리키며) "JetBot의 눈, 카메라를 통해 실시간으로 주변을 볼 수 있어요."
    *   (버튼 가리키며) "전진, 후진, 좌회전, 우회전, 정지!  간단한 버튼 클릭만으로 JetBot을 움직일 수 있죠."
    *   (응답 텍스트 가리키며) "Ollama가 분석한 내용이 여기에 텍스트로 표시됩니다."
    *   (오디오 플레이어 가리키며) "그리고 Edge TTS가 이 내용을 음성으로 읽어주죠!"
    *   (데모) "주변 설명" 버튼 클릭 -> JetBot이 주변 상황 설명 (TTS 음성)
    *   (데모) "앞으로" 버튼 클릭 -> JetBot 전진
    *   (데모) "장애물을 피해서 이동해줘." 텍스트 입력 후 "실행" -> JetBot 장애물 회피
    *   "이 모든 게 PC에서 원격으로, 실시간으로 이루어진다는 게 놀랍지 않나요?"

**(1:30 - 3:00) 구성 요소 소개**

*   화면:  JetBot, Jetson Nano, PC, Ollama, FastAPI, Edge TTS 로고 및 이미지. 각 구성 요소를 연결하는 다이어그램.
*   내레이션:
    *   "이 프로젝트는 크게 5가지 구성 요소로 이루어져 있어요."
    *   "먼저, 우리의 주인공 **JetBot**!  NVIDIA Jetson Nano 개발자 키트를 기반으로 만들어진 로봇이죠." (JetBot 클로즈업)
    *   "JetBot의 두뇌 역할을 하는 **Jetson Nano**!  작지만 강력한 성능으로 이미지 처리, 딥러닝 추론 등을 수행합니다." (Jetson Nano 클로즈업)
    *   "그리고 이 모든 것을 제어하는 **PC**!  여기에 FastAPI 웹 서버를 구축하고, Ollama를 실행해서 JetBot과 통신할 거예요." (PC 화면)
    *   "**Ollama**는 대규모 언어 모델(LLM)을 로컬에서 쉽게 사용할 수 있게 해주는 도구입니다.  JetBot이 보는 것을 이해하는 데 사용되죠." (Ollama 로고)
    *   "**FastAPI**는 빠르고 간편하게 웹 API를 만들 수 있게 해주는 파이썬 프레임워크입니다.  JetBot과 PC 간의 통신을 담당하죠." (FastAPI 로고)
    *   "마지막으로 **Edge TTS**!  마이크로소프트의 텍스트 음성 변환 기술로,  Ollama의 분석 결과를 자연스러운 한국어 음성으로 바꿔줍니다." (Edge TTS 로고)
    *   (다이어그램) "이 구성 요소들이 서로 어떻게 연결되는지 그림으로 볼까요?  JetBot의 카메라 -> Ollama -> FastAPI -> PC (웹 브라우저, TTS) -> JetBot (명령) 순서로 데이터가 흐릅니다."

**(3:00 - 6:00) 설치 및 설정**

*   화면:  터미널 창, 코드 편집기, 웹 브라우저 화면 등을 번갈아 보여줌.
*   내레이션:
    *   "자, 이제 본격적으로 설치를 시작해 볼까요?  먼저, JetBot과 Jetson Nano는 이미 설정되어 있다고 가정할게요.  JetBot 조립 및 설정 방법은 이 영상에서는 다루지 않지만,  관련 자료는 더보기 란에 링크해 두겠습니다."
    *   "PC에 Python이 설치되어 있어야 하고요,  가상 환경을 사용하는 것을 추천합니다."
    *   (터미널 창) "먼저, Ollama를 설치해야 합니다.  Ollama 웹사이트에서 다운로드 받아서 설치하면 돼요.  설치가 완료되면, 터미널에서 `ollama run gemma3:4b` 명령을 실행해서 gemma3:4b 모델을 다운로드 받습니다."
    *   (코드 편집기) "다음은, 이 프로젝트 코드를 다운로드 받을 차례입니다.  깃허브에서 클론하거나, ZIP 파일로 다운로드 받으세요.  링크는 더보기 란에 있습니다."
    *   "다운로드 받은 폴더로 이동해서,  `pip install -r requirements.txt` 명령으로 필요한 파이썬 패키지들을 설치합니다."
    *    "이제 `main.py` 파일을 열어서 설정을 확인해 볼까요?  `OLLAMA_HOST`, `MODEL_NAME`, `JETBOT_WEBSOCKET_URL`, `VOICE` 변수들을 확인하고,  필요하면 여러분의 환경에 맞게 수정하세요. 특히, `JETBOT_WEBSOCKET_URL`은 JetBot의 IP 주소와 포트 번호로 바꿔줘야 합니다."
        *  (코드 하이라이트 하면서 설명)
    * "JetBot에서는 `jetbot_control.py` 파일을 실행합니다."
    *   (터미널 창) "PC에서 FastAPI 서버를 실행합니다.  `uvicorn main:app --reload --port 8000 --host 0.0.0.0` 명령을 사용하면 돼요."
    *   (웹 브라우저) "웹 브라우저를 열고, `http://localhost:8000` (또는 PC의 IP 주소:8000)으로 접속하면,  JetBot 제어 센터가 나타날 겁니다!"

**(6:00 - 8:00) 코드 설명 (FastAPI - main.py)**

*   화면:  `main.py` 코드.  핵심 부분 (함수, 클래스)을 확대해서 보여줌.
*   내레이션:
    *   "이제 코드를 살펴볼까요?  먼저, FastAPI 서버 역할을 하는 `main.py` 코드입니다."
    *   (Pydantic 모델) "`OllamaRequest` 모델은 PC에서 Ollama로 보낼 요청 데이터를 정의합니다.  프롬프트, 이미지, 액션, 방향 힌트 등을 담을 수 있죠."
    *   (`query_ollama_stream` 함수) "이 함수는 Ollama API에 요청을 보내고, 스트리밍 방식으로 응답을 받습니다.  응답이 JSON 형식이면 `response` 키의 값을, 아니면 전체 응답을 반환하죠. 중간에 한글 깨짐을 방지하는 처리도 되어있습니다."
    *    (speak함수) "엣지 tts로 음성을 생성하는 부분입니다."
    *   (`send_command_to_jetbot` 함수) "이 함수는 JetBot에게 명령을 보내는 역할을 합니다.  웹소켓을 통해 JSON 형식의 메시지를 보내죠.  연결에 실패하거나 타임아웃이 발생하면 예외를 처리합니다."
    *   (`generate` 함수) "이게 핵심 함수인데요,  사용자의 요청을 받아서 Ollama에 질의하고,  JetBot에게 명령을 보내고,  TTS로 음성을 생성하는 모든 작업을 처리합니다."
        *   (단계별로 설명) "먼저, 이미지 데이터를 받아서 Ollama에 전달하고,  응답이 오면 JetBot 명령을 결정하고, JetBot에 명령을 보내고, 마지막으로 TTS를 생성해서 결과를 반환합니다."
        * (tts_text 변수) "상황에 따라서 다른 tts를 출력하도록 수정"

**(8:00 - 9:30) 코드 설명 (JetBot - jetbot_control.py)**

*   화면:  `jetbot_control.py` 코드.
*   내레이션:
    *   "이번에는 JetBot에서 실행되는 `jetbot_control.py` 코드를 살펴보겠습니다."
    *   (handle_command 함수) "이 함수는 PC에서 보낸 명령을 받아서 JetBot을 실제로 움직이는 역할을 합니다.  `forward_medium`, `backward_slow`와 같이 미리 정의된 동작들을 수행하죠."
    * (forward, backward, left, right): 속도와 시간을 파라미터로
    * (avoid_obstacle, rotate): 파라미터 없이
    *   (웹소켓 관련 함수) `websocket_handler` 함수는 웹소켓 연결을 처리하고,  `send_images` 함수는 JetBot 카메라 이미지를 PC로 계속 전송합니다."
    *   (main 함수) "이 함수는 웹소켓 서버를 시작하는 역할을 합니다."

**(9:30 - 10:30) 추가 기능 및 확장 가능성**

*   화면:  다양한 활용 예시 (얼굴 인식, 객체 추적, 지도 작성, 음성 명령 등)
*   내레이션:
    *   "이 프로젝트는 여기서 끝이 아닙니다!  다양한 기능을 추가해서 더욱 멋진 로봇을 만들 수 있어요."
    *   "예를 들어, 얼굴 인식 기능을 추가해서 등록된 사용자만 JetBot을 제어하게 할 수도 있고,"
    *   "특정 물체를 인식하고 따라다니게 할 수도 있죠."
    *   "SLAM 기술을 이용해서 주변 환경 지도를 만들고,  JetBot이 스스로 목적지를 찾아가게 할 수도 있습니다."
    *   "음성 명령 인식 기능을 추가하면,  말로 JetBot을 제어할 수 있게 됩니다.  'JetBot, 앞으로 가!', '저 물건 뭐야?' 처럼요!"

**(10:30 - 11:00) 마무리**

*   화면:  JetBot 프로젝트의 다양한 활용 예시,  제작자 정보
*   내레이션:
    *   "자, 오늘 준비한 내용은 여기까지입니다.  JetBot, Ollama, FastAPI, Edge TTS를 활용해서 원격 제어와 실시간 상황 인식이 가능한 로봇 시스템을 함께 만들어봤는데요,  어떠셨나요?"
    *   "이 프로젝트가 여러분의 로봇 개발 여정에 작은 도움이 되었으면 좋겠습니다."
    *   "궁금한 점이 있다면 언제든지 댓글로 질문해주세요!"
    *   "구독과 좋아요, 알림 설정도 잊지 마시고요!  다음 영상에서 또 만나요!"

**(Optional) 추가 영상 (5분 내외)**

*   **문제 해결 (Troubleshooting):**  자주 발생하는 문제와 해결 방법을 소개합니다.  (예: 한글 깨짐, TTS 오류, JetBot 연결 오류, 504 Gateway Timeout 등)
*   **JetBot 조립 및 설정:**  JetBot 조립 및 설정 과정을 간략하게 소개합니다. (별도 영상 링크 제공)
*   **코드 상세 설명:**  코드의 각 부분을 더 자세하게 설명합니다.

**참고:**

*   영상 중간중간에 적절한 시각 자료 (이미지, 동영상, 다이어그램, 코드 스니펫 등)를 활용하여 시청자의 이해를 돕습니다.
*   필요한 경우, 자막을 추가합니다.
*   배경 음악과 효과음을 적절히 사용합니다.
*   시청자의 참여를 유도하는 질문을 던지거나, 댓글 이벤트를 진행합니다.

이 대본은 참고용이며,  실제 영상 제작 시에는 내용과 구성을 자유롭게 변경할 수 있습니다.
